{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Energy Data Exploration\n",
        "\n",
        "This notebook explores energy meter data from two different meters (identified by their MUIDs) for the period of February 2023.\n",
        "\n",
        "## Objectives\n",
        "1. Load and understand the data structure\n",
        "2. Group data by different time intervals (hourly, daily, weekly)\n",
        "3. Identify patterns (day/night, weekday/weekend)\n",
        "4. Form a hypothesis about what the data represents\n",
        "5. Perform autocorrelation analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run this cell first if packages are missing)\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# API endpoints (S3 JSON files simulating an API)\n",
        "METER_1_URL = \"https://exnaton-public-s3-bucket20230329123331528000000001.s3.eu-central-1.amazonaws.com/challenge/95ce3367-cbce-4a4d-bbe3-da082831d7bd.json\"\n",
        "METER_2_URL = \"https://exnaton-public-s3-bucket20230329123331528000000001.s3.eu-central-1.amazonaws.com/challenge/1db7649e-9342-4e04-97c7-f0ebb88ed1f8.json\"\n",
        "\n",
        "print(\"Fetching data from API...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch data from both endpoints\n",
        "response_1 = requests.get(METER_1_URL)\n",
        "response_2 = requests.get(METER_2_URL)\n",
        "\n",
        "data_1 = response_1.json()\n",
        "data_2 = response_2.json()\n",
        "\n",
        "print(f\"Meter 1 response status: {response_1.status_code}\")\n",
        "print(f\"Meter 2 response status: {response_2.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrames\n",
        "# Data structure: dict with 'data' key containing list of measurements\n",
        "# Each measurement has: measurement, timestamp, tags, and a value column (OBIS code)\n",
        "\n",
        "df_1 = pd.DataFrame(data_1['data'])\n",
        "df_2 = pd.DataFrame(data_2['data'])\n",
        "\n",
        "# OBIS codes explanation:\n",
        "# 0100011D00FF = Active energy import (consumption from grid)\n",
        "# 0100021D00FF = Active energy export (production to grid, e.g., solar)\n",
        "\n",
        "# Rename value columns to standardized names and add meter labels\n",
        "df_1['value'] = df_1['0100011D00FF']\n",
        "df_1['meter'] = 'Meter 1 - Import (Consumption)'\n",
        "df_1['meter_type'] = 'consumption'\n",
        "\n",
        "df_2['value'] = df_2['0100021D00FF']\n",
        "df_2['meter'] = 'Meter 2 - Export (Production)'\n",
        "df_2['meter_type'] = 'production'\n",
        "\n",
        "print(f\"Meter 1 shape: {df_1.shape}\")\n",
        "print(f\"Meter 2 shape: {df_2.shape}\")\n",
        "print(f\"\\nMeter 1 columns: {df_1.columns.tolist()}\")\n",
        "print(f\"Meter 2 columns: {df_2.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect Meter 1 data\n",
        "print(\"=== Meter 1 Data ===\")\n",
        "print(f\"\\nColumns: {df_1.columns.tolist()}\")\n",
        "print(f\"\\nData types:\\n{df_1.dtypes}\")\n",
        "df_1.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect Meter 2 data\n",
        "print(\"=== Meter 2 Data ===\")\n",
        "print(f\"\\nColumns: {df_2.columns.tolist()}\")\n",
        "print(f\"\\nData types:\\n{df_2.dtypes}\")\n",
        "df_2.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine both datasets for unified analysis\n",
        "df_combined = pd.concat([df_1, df_2], ignore_index=True)\n",
        "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
        "df_combined.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== Missing Values ===\")\n",
        "print(df_combined.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "print(\"=== Statistical Summary ===\")\n",
        "df_combined.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse timestamps - adjust column name based on actual data\n",
        "# Common timestamp column names: 'timestamp', 'time', 'datetime', 'date'\n",
        "timestamp_cols = [col for col in df_combined.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
        "print(f\"Potential timestamp columns: {timestamp_cols}\")\n",
        "\n",
        "# Will use the first found timestamp column, or adjust as needed\n",
        "if timestamp_cols:\n",
        "    ts_col = timestamp_cols[0]\n",
        "    df_combined['timestamp'] = pd.to_datetime(df_combined[ts_col])\n",
        "    df_combined = df_combined.sort_values('timestamp')\n",
        "    print(f\"\\nDate range: {df_combined['timestamp'].min()} to {df_combined['timestamp'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Time-Based Grouping & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify the value column (likely 'value', 'energy', 'consumption', 'power')\n",
        "# Adjust based on actual column names discovered above\n",
        "value_cols = [col for col in df_combined.columns if col.lower() in ['value', 'energy', 'consumption', 'power', 'reading']]\n",
        "print(f\"Potential value columns: {value_cols}\")\n",
        "\n",
        "# Also check numeric columns\n",
        "numeric_cols = df_combined.select_dtypes(include=['number']).columns.tolist()\n",
        "print(f\"Numeric columns: {numeric_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add time components for grouping\n",
        "if 'timestamp' in df_combined.columns:\n",
        "    df_combined['hour'] = df_combined['timestamp'].dt.hour\n",
        "    df_combined['day_of_week'] = df_combined['timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "    df_combined['day_name'] = df_combined['timestamp'].dt.day_name()\n",
        "    df_combined['date'] = df_combined['timestamp'].dt.date\n",
        "    df_combined['week'] = df_combined['timestamp'].dt.isocalendar().week\n",
        "    df_combined['is_weekend'] = df_combined['day_of_week'].isin([5, 6])\n",
        "    \n",
        "    print(\"Time components added:\")\n",
        "    df_combined[['timestamp', 'hour', 'day_name', 'is_weekend', 'week']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raw time series visualization\n",
        "# NOTE: Adjust 'value_column' to the actual column name from your data\n",
        "value_column = 'value'  # CHANGE THIS based on actual data structure\n",
        "\n",
        "fig = px.line(\n",
        "    df_combined, \n",
        "    x='timestamp', \n",
        "    y=value_column,\n",
        "    color='meter',\n",
        "    title='Energy Data Time Series - Full Period',\n",
        "    labels={value_column: 'Energy Value', 'timestamp': 'Date/Time'}\n",
        ")\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hourly patterns (Day/Night analysis)\n",
        "hourly_avg = df_combined.groupby(['meter', 'hour'])[value_column].mean().reset_index()\n",
        "\n",
        "fig = px.line(\n",
        "    hourly_avg,\n",
        "    x='hour',\n",
        "    y=value_column,\n",
        "    color='meter',\n",
        "    title='Average Energy by Hour of Day (Day/Night Pattern)',\n",
        "    labels={value_column: 'Average Energy', 'hour': 'Hour of Day (0-23)'}\n",
        ")\n",
        "fig.add_vrect(x0=6, x1=18, fillcolor='yellow', opacity=0.1, annotation_text='Daylight Hours')\n",
        "fig.update_layout(height=400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Day of Week patterns\n",
        "daily_avg = df_combined.groupby(['meter', 'day_of_week', 'day_name'])[value_column].mean().reset_index()\n",
        "# Sort by day of week\n",
        "daily_avg = daily_avg.sort_values('day_of_week')\n",
        "\n",
        "fig = px.bar(\n",
        "    daily_avg,\n",
        "    x='day_name',\n",
        "    y=value_column,\n",
        "    color='meter',\n",
        "    barmode='group',\n",
        "    title='Average Energy by Day of Week',\n",
        "    labels={value_column: 'Average Energy', 'day_name': 'Day'},\n",
        "    category_orders={'day_name': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']}\n",
        ")\n",
        "fig.update_layout(height=400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weekday vs Weekend comparison\n",
        "weekend_comparison = df_combined.groupby(['meter', 'is_weekend'])[value_column].agg(['mean', 'std', 'sum']).reset_index()\n",
        "weekend_comparison['period'] = weekend_comparison['is_weekend'].map({True: 'Weekend', False: 'Weekday'})\n",
        "\n",
        "fig = px.bar(\n",
        "    weekend_comparison,\n",
        "    x='period',\n",
        "    y='mean',\n",
        "    color='meter',\n",
        "    barmode='group',\n",
        "    error_y='std',\n",
        "    title='Weekday vs Weekend Average Energy',\n",
        "    labels={'mean': 'Average Energy', 'period': 'Period'}\n",
        ")\n",
        "fig.update_layout(height=400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daily totals over time\n",
        "daily_totals = df_combined.groupby(['meter', 'date'])[value_column].sum().reset_index()\n",
        "\n",
        "fig = px.line(\n",
        "    daily_totals,\n",
        "    x='date',\n",
        "    y=value_column,\n",
        "    color='meter',\n",
        "    title='Daily Total Energy',\n",
        "    labels={value_column: 'Total Energy', 'date': 'Date'}\n",
        ")\n",
        "fig.update_layout(height=400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weekly totals\n",
        "weekly_totals = df_combined.groupby(['meter', 'week'])[value_column].sum().reset_index()\n",
        "\n",
        "fig = px.bar(\n",
        "    weekly_totals,\n",
        "    x='week',\n",
        "    y=value_column,\n",
        "    color='meter',\n",
        "    barmode='group',\n",
        "    title='Weekly Total Energy (February 2023)',\n",
        "    labels={value_column: 'Total Energy', 'week': 'Week Number'}\n",
        ")\n",
        "fig.update_layout(height=400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap: Hour of Day vs Day of Week\n",
        "for meter_name in df_combined['meter'].unique():\n",
        "    meter_data = df_combined[df_combined['meter'] == meter_name]\n",
        "    pivot = meter_data.pivot_table(\n",
        "        values=value_column, \n",
        "        index='hour', \n",
        "        columns='day_name', \n",
        "        aggfunc='mean'\n",
        "    )\n",
        "    # Reorder columns\n",
        "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    pivot = pivot[[col for col in day_order if col in pivot.columns]]\n",
        "    \n",
        "    fig = px.imshow(\n",
        "        pivot,\n",
        "        title=f'Energy Heatmap: {meter_name}',\n",
        "        labels=dict(x='Day of Week', y='Hour of Day', color='Avg Energy'),\n",
        "        aspect='auto'\n",
        "    )\n",
        "    fig.update_layout(height=500)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Hypothesis Formation\n",
        "\n",
        "### Key Discovery: OBIS Codes Reveal Data Type\n",
        "\n",
        "The two meters use different OBIS codes (standard energy metering identifiers):\n",
        "- **Meter 1** (`0100011D00FF`): Active energy **import** = Energy consumed FROM the grid\n",
        "- **Meter 2** (`0100021D00FF`): Active energy **export** = Energy sent TO the grid (e.g., solar)\n",
        "\n",
        "### Observations from Data Analysis\n",
        "\n",
        "**Data Characteristics:**\n",
        "- Date range: February 1-28, 2023\n",
        "- Measurement interval: 15 minutes (96 readings/day)\n",
        "- 2,688 data points per meter\n",
        "\n",
        "**Meter 1 - Consumption Patterns:**\n",
        "- Mean: 0.036 kWh per 15-min interval\n",
        "- Total February consumption: ~97.6 kWh\n",
        "- Peaks in evening hours (18-19h, 22h) - typical household usage\n",
        "- Slightly higher on weekends (people home more)\n",
        "\n",
        "**Meter 2 - Production Patterns:**\n",
        "- Mean: 0.014 kWh per 15-min interval\n",
        "- Total February production: ~36.2 kWh\n",
        "- **Zero production at night (17h-6h)** - confirms solar\n",
        "- Peaks at midday (10-12h) - classic solar irradiance curve\n",
        "- Production varies by day (weather-dependent)\n",
        "\n",
        "### Confirmed Hypothesis\n",
        "\n",
        "This data represents a **residential prosumer** (producer + consumer):\n",
        "- **Meter 1**: Household energy consumption from the grid\n",
        "- **Meter 2**: Solar panel energy production exported to grid\n",
        "\n",
        "The household consumes ~97.6 kWh but produces ~36.2 kWh from solar, offsetting roughly 37% of their consumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Autocorrelation Analysis (Bonus)\n",
        "\n",
        "Autocorrelation helps identify repeating patterns in time-series data:\n",
        "- Strong autocorrelation at lag 24: Daily patterns (if hourly data)\n",
        "- Strong autocorrelation at lag 168: Weekly patterns (if hourly data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Autocorrelation analysis for each meter\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "for idx, meter_name in enumerate(df_combined['meter'].unique()):\n",
        "    meter_data = df_combined[df_combined['meter'] == meter_name].sort_values('timestamp')\n",
        "    series = meter_data[value_column].dropna()\n",
        "    \n",
        "    # ACF (Autocorrelation Function)\n",
        "    plot_acf(series, lags=50, ax=axes[idx, 0], title=f'ACF - {meter_name}')\n",
        "    \n",
        "    # PACF (Partial Autocorrelation Function)\n",
        "    plot_pacf(series, lags=50, ax=axes[idx, 1], title=f'PACF - {meter_name}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extended autocorrelation to check for weekly patterns\n",
        "# Adjust lags based on actual data frequency\n",
        "\n",
        "for meter_name in df_combined['meter'].unique():\n",
        "    meter_data = df_combined[df_combined['meter'] == meter_name].sort_values('timestamp')\n",
        "    series = meter_data[value_column].dropna()\n",
        "    \n",
        "    # Calculate time interval\n",
        "    time_diff = meter_data['timestamp'].diff().median()\n",
        "    print(f\"\\n{meter_name}:\")\n",
        "    print(f\"  Data frequency: {time_diff}\")\n",
        "    print(f\"  Total data points: {len(series)}\")\n",
        "    \n",
        "    # Calculate lags for daily and weekly patterns\n",
        "    if time_diff:\n",
        "        points_per_day = int(pd.Timedelta('1 day') / time_diff)\n",
        "        print(f\"  Points per day: {points_per_day}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Data represents**: A residential **prosumer** with both consumption and solar production\n",
        "   - **Meter 1**: Grid import (consumption) - OBIS 0100011D00FF\n",
        "   - **Meter 2**: Grid export (solar production) - OBIS 0100021D00FF\n",
        "\n",
        "2. **Temporal patterns identified**:\n",
        "   - **Daily seasonality**: \n",
        "     - Production: Strong daily pattern following solar irradiance curve (zero at night, peak at noon)\n",
        "     - Consumption: Evening/night peaks typical of households\n",
        "   - **Weekly seasonality**: \n",
        "     - Consumption slightly higher on weekends (residents home more)\n",
        "     - Production independent of day-of-week (weather-driven)\n",
        "\n",
        "3. **Energy balance**:\n",
        "   - February consumption: ~97.6 kWh\n",
        "   - February production: ~36.2 kWh  \n",
        "   - Solar offset: ~37% of consumption\n",
        "\n",
        "### Autocorrelation Insights\n",
        "\n",
        "**Production (Solar):**\n",
        "- Very high short-term autocorrelation (0.99 at lag 1) - solar output changes gradually\n",
        "- Strong negative correlation at 12-hour lag (-0.21) - day vs night\n",
        "- Moderate daily seasonality (0.42 at 24h lag) - weather varies day to day\n",
        "\n",
        "**Consumption:**\n",
        "- Moderate short-term correlation (0.50 at lag 1)\n",
        "- Rapid decay to near-zero - consumption is more stochastic\n",
        "- Weak daily pattern - household activities vary\n",
        "\n",
        "### Implications for Backend/Frontend\n",
        "- Data granularity: 15-minute intervals (good for detailed analysis)\n",
        "- API should support time-range queries and aggregation options (hourly, daily, weekly)\n",
        "- Frontend should highlight the complementary nature of consumption vs production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics by meter to help form hypothesis\n",
        "summary = df_combined.groupby('meter')[value_column].agg(['mean', 'std', 'min', 'max', 'sum'])\n",
        "print(\"=== Summary by Meter ===\")\n",
        "summary"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
